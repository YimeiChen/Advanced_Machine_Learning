{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_ds = datasets.MNIST('data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#batch_size = 5 # for testing\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "train_dl = iter(train_loader)\n",
    "x, y = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(img, title=None):\n",
    "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n",
    "    if title is not None: plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first from torch to numpy\n",
    "X = x.numpy(); Y = y.numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7188516b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(X[0][0], Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NN model skeleton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader, test_loader, num_epochs, model, optimizer):\n",
    "    model.train()\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            batch = images.shape[0] # size of the batch\n",
    "            # Convert torch tensor to Variable, change shape of the input\n",
    "            images = Variable(images.view(-1, 28*28)).cuda() #sending to GPU\n",
    "            labels = Variable(labels).cuda()\n",
    "        \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels) #torch.nn.functional all these can be called as F\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            total += batch\n",
    "            sum_loss += batch * loss.data[0]\n",
    "            #if (i+1) % 100 == 0:\n",
    "                #print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                 #  %(epoch+1, num_epochs, sum_loss/total))\n",
    "                \n",
    "        train_loss = sum_loss/total\n",
    "        #print('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, train_loss))\n",
    "        val_acc, val_loss = model_accuracy_loss(model, test_loader)\n",
    "        #print('Epoch [%d/%d], Valid Accuracy: %.4f, Valid Loss: %.4f' %(epoch+1, num_epochs, val_acc, val_loss))\n",
    "    return val_acc, val_loss, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_accuracy_loss(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        outputs = model(images)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        sum_loss += labels.size(0)*loss.data[0]\n",
    "        total += labels.size(0)\n",
    "        correct += pred.eq(labels.data).cpu().sum()\n",
    "    return 100 * correct / total, sum_loss/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for the number of neurons in the hidden unit\n",
    "def get_model(M = 300):\n",
    "    net = nn.Sequential(nn.Linear(28*28, M),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(M, 10))\n",
    "    return net.cuda()\n",
    "# last is M, 10, ten digits\n",
    "# input is 28*28 input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Tune the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 11.71 val_loss: 2.4993606101989747 train_loss: 15.69046331932068\n",
      "val_acc: 10.3 val_loss: 2.3068014167785647 train_loss: 2.346710188153585\n",
      "val_acc: 94.95 val_loss: 0.2701721287965774 train_loss: 0.18489152574996154\n",
      "val_acc: 97.93 val_loss: 0.08764553509950637 train_loss: 0.0554013519248118\n",
      "val_acc: 97.58 val_loss: 0.07912932171821595 train_loss: 0.1581827309225003\n",
      "val_acc: 92.7 val_loss: 0.2543579046726227 train_loss: 0.46689417521476745\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rate:\n",
    "    net = get_model()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    model_accuracy_loss(net, test_loader)\n",
    "    val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    print(\"val_acc:\",val_acc,\"val_loss:\", val_loss,\"train_loss:\",train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see from both validation accuracy and validation loss point of view, 0.01 and 0.001 learning rate yield top 2 results. I will interpolate between these two value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate2 = [0.01,0.004,0.008, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.01 val_acc: 95.39 val_loss: 0.23787590246200563\n",
      "lr: 0.004 val_acc: 96.82 val_loss: 0.19191578224599362\n",
      "lr: 0.008 val_acc: 95.91 val_loss: 0.23701444540023803\n",
      "lr: 0.001 val_acc: 98.11 val_loss: 0.0758550889492035\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rate2:\n",
    "    net = get_model()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    model_accuracy_loss(net, test_loader)\n",
    "    val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    print(\"lr:\", lr,\"val_acc:\",val_acc,\"val_loss:\",val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OBSERVATION:*: \n",
    "\n",
    "The best validation accuracy is when learning rate is 0.001. From the experiment in the last round, we can see that compared to 0.01, 0.001 yields smaller training and validation loss so overfitting is not a problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Tune the hidden layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_size = [10, 50, 100, 300, 1000, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_size: 10 val_acc: 90.03 val_loss: 0.33734149022102355 train_loss: 0.3717853285161654\n",
      "layer_size: 50 val_acc: 95.07 val_loss: 0.22485833089351653 train_loss: 0.193383240087231\n",
      "layer_size: 100 val_acc: 94.76 val_loss: 0.27137517743110656 train_loss: 0.1831809880844752\n",
      "layer_size: 300 val_acc: 95.1 val_loss: 0.2450180930495262 train_loss: 0.1829553218181928\n",
      "layer_size: 1000 val_acc: 95.25 val_loss: 0.2694191605567932 train_loss: 0.19047950228353341\n",
      "layer_size: 2000 val_acc: 95.47 val_loss: 0.25217795172929763 train_loss: 0.1887618602490425\n"
     ]
    }
   ],
   "source": [
    "for s in layer_size:\n",
    "    net = get_model(M = s )\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "    model_accuracy_loss(net, test_loader)\n",
    "    val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    print(\"layer_size:\", s, \"val_acc:\",val_acc,\"val_loss:\", val_loss,\"train_loss:\",train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OBSERVATION:*\n",
    "\n",
    "the layer size that yielded the best accuracy is 2000. Overfitting start to show up beyond layer size of 50 because validation loss reached the minimum at 0.22 and start to increase since that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Tune the L2 regularization weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_decay = [0, 0.0001, 0.001, 0.01, 0.1, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight decay: 0 val_acc: 98.03 val_loss: 0.12329273672103881 train_loss: 0.03379573976036161\n",
      "weight decay: 0.0001 val_acc: 97.75 val_loss: 0.0767269659459591 train_loss: 0.039960156274487575\n",
      "weight decay: 0.001 val_acc: 98.03 val_loss: 0.06791373198032379 train_loss: 0.0709504307956497\n",
      "weight decay: 0.01 val_acc: 96.37 val_loss: 0.1430223176717758 train_loss: 0.16924466012020906\n",
      "weight decay: 0.1 val_acc: 89.77 val_loss: 0.44151821737289426 train_loss: 0.47092641704559324\n",
      "weight decay: 0.3 val_acc: 85.09 val_loss: 0.7888756879806519 train_loss: 0.819635253944397\n"
     ]
    }
   ],
   "source": [
    "for wd in weight_decay:\n",
    "    net = get_model(M = 300 )\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay =wd)\n",
    "    model_accuracy_loss(net, test_loader)\n",
    "    val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs=20, model=net, optimizer=optimizer)\n",
    "    print(\"weight decay:\", wd, \"val_acc:\",val_acc,\"val_loss:\", val_loss,\"train_loss:\",train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. models with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_v2(M = 300, p=0):\n",
    "    modules = []\n",
    "    modules.append(nn.Linear(28*28, M))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(M, 10))\n",
    "    return nn.Sequential(*modules).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plist = [0,0.3,0.5,0.7,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout rate: 0 val_acc: 97.93 val_loss: 0.11732124398350716 train_loss: 0.033310068961158394\n",
      "dropout rate: 0.3 val_acc: 97.99 val_loss: 0.12572206707298755 train_loss: 0.03589762788457175\n",
      "dropout rate: 0.5 val_acc: 97.97 val_loss: 0.1145123995423317 train_loss: 0.039802622276532154\n",
      "dropout rate: 0.7 val_acc: 97.88 val_loss: 0.13438771730959415 train_loss: 0.04717220964011426\n",
      "dropout rate: 1 val_acc: 97.84 val_loss: 0.11502244354486466 train_loss: 0.14324818946938342\n"
     ]
    }
   ],
   "source": [
    "for p in plist:\n",
    "    net = get_model_v2(M = 300, p=p)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    val_acc, val_loss, train_loss=train_model(train_loader, test_loader, num_epochs=20, model=net, optimizer=optimizer)\n",
    "    print(\"dropout rate:\", p, \"val_acc:\",val_acc,\"val_loss:\", val_loss,\"train_loss:\",train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OBSERVATION:*\n",
    "\n",
    "If we use accuracy to measure the model performance, the best performance is achieved when dropout ratio equals 0.3. At this small ratio, a small portion of the nerons were zeroed out so that most nerons still functioned to make the prediction.\n",
    "\n",
    "With dropout method, the model generally resulted a higher validation accuracy so we can conclude that dropout help to increase testing accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. 3-layer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_v3(M1 = 30, M2 = 40, p=0):\n",
    "    modules = []\n",
    "    modules.append(nn.Linear(28*28, M1))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(M1, M2))\n",
    "    modules.append(nn.Linear(M2, 10))\n",
    "    return nn.Sequential(*modules).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout rate: 0 weight: 0 learning rate: 0.1 val_loss: 2.3913672847747804 train_loss: 2.3381452978960673\n",
      "dropout rate: 0 weight: 0 learning rate: 0.001 val_loss: 0.14729523298740388 train_loss: 0.08845896285573641\n",
      "dropout rate: 0.3 weight: 0 learning rate: 0.1 val_loss: 2.3756185348510743 train_loss: 2.3319020805040993\n",
      "dropout rate: 0.3 weight: 0 learning rate: 0.001 val_loss: 0.15349675698280335 train_loss: 0.10694206710656484\n",
      "dropout rate: 1 weight: 0 learning rate: 0.1 val_loss: 2.3436830627441405 train_loss: 2.506178075358073\n",
      "dropout rate: 1 weight: 0 learning rate: 0.001 val_loss: 0.13687180075645447 train_loss: 0.20962918531278768\n",
      "dropout rate: 0 weight: 0.3 learning rate: 0.1 val_loss: 1.5694501695632934 train_loss: 2.796901769323349\n",
      "dropout rate: 0 weight: 0.3 learning rate: 0.001 val_loss: 1.1922107063293457 train_loss: 1.2018240967877707\n",
      "dropout rate: 0.3 weight: 0.3 learning rate: 0.1 val_loss: 1.4958884099960328 train_loss: 2.7230308541742962\n"
     ]
    }
   ],
   "source": [
    "L2_weight = [0, 0.3]\n",
    "plist =[0,0.3,1]\n",
    "lr_list = [0.1,0.001]\n",
    "for wd in L2_weight:\n",
    "    for p in plist:\n",
    "        for lr in lr_list:\n",
    "            net = get_model_v3(p=p)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr,weight_decay =wd)\n",
    "            val_acc, val_loss, train_loss=train_model(train_loader, test_loader, num_epochs=20, model=net, optimizer=optimizer)\n",
    "            print(\"dropout rate:\", p, \"weight:\", wd,\"learning rate:\", lr,\"val_loss:\", val_loss,\"train_loss:\",train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*OBSERVATION*\n",
    "\n",
    "To avoid overfitting in the parameter grid search process, I decided to use validation loss to assess the performance of the models. The smallest validation loss is 0.13 when dropout rate is 1, weight is 0 (no regularization) and learning rate is 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
